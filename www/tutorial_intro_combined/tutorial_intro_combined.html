<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>tutorial_intro_combined</TITLE>
<META NAME="description" CONTENT="tutorial_intro_combined">
<META NAME="keywords" CONTENT="tutorial_intro_combined">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="tutorial_intro_combined.css">

<LINK REL="next" HREF="node1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html2"
  HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev_g.png">   
<BR>
<B> Next:</B> <A NAME="tex2html3"
  HREF="node1.html">About this document ...</A>
<BR>
<BR>
<!--End of Navigation Panel-->
<!--This document is a combination of "A Tutorial Introduction to PEP" and "TEA for survey processing". Both documents have important information regarding the usage of TEA and this document is meant to combine that information into one source. Clean up and correct as you see fit.
par
- DV -->
<h2>U.S. Census Bureau</h2>
<BR>
<h2>Center for Statistical Research and Methodology Division</h2>
<h1>TEA for survey processing</h1>

<p class="digression">TEA is a system designed to unify and streamline survey processing, from raw data to
editing to imputation to dissemination of output. Its primary focus is in finding 
observations that are missing data, fail consistency checks, or risk the disclosure of
sensitive information, and then using a unified imputation process to fix all of these
issues. Beyond this central focus, it includes tools for reading in data, generating
fully synthetic data, and other typical needs of the survey processor.</p>

<h3>Overview</h3>

<p>We intend to implement the many steps of survey processing into a single
framework, where the interface with which analysts work is common across surveys,
the full description of how a survey is processed is summarized in one place, and the
code implementing the procedures are internally well-documented and reasonably easy
to maintain.</p>

<p>Raw data is often rife with missing items, logical errors, and sensitive information. 
To ignore these issues risks alienating respondents and data users alike, and so data 
modification is a necessary part of the production of quality survey and census data. 
TEA is a statistical library designed to eradicate these errors by creating a framework 
that is friendly to interact with, effectively handles Census-sized data sets, and does 
the processing quickly even with relatively sophisticated methods.</p>

<p>This paper gives a detailed overview of the TEA system, its components, and how they're 
used. If you have this tutorial you should also have a working installation of TEA and 
a sample directory including <tt>demo.spec</tt>, <tt>demo.R</tt>, and <tt>dc_pums_08.csv</tt>. 
Basic versions of all of the steps described below are already implemented and running,
though the system will evolve and grow as it is applied in new surveys and environments.</p>

<h3>System basics</h3>
<p>TEA implements a two step process for addressing issues with raw data. The first is to
identify those failures listed above (missing data, logical errors, or sensitive
information), and then, having identified problem data, impute new values
to replace the old ones. Although the term <i>imputation</i> is typically used only to 
describe filling in missing data, we use it broadly to mean any modification of a 
data item that involves choosing among alternatives, regardless of which of the 
above failures prompted the fill-in.</p>

<p>In terms of how TEA is implemented, we break the process into two parts: the 
specification of variable details--such as which file to read in, what values should 
be top-coded, or the full description of the model used for filling in missing 
data--and the actual prodecure to be run by the computer based on those inputs. 
The specification of details as mentioned above will go into a plain text file, 
herein called the <tt>spec</tt> file. Based on your inputs to the <tt>spec</tt> file 
(which we will explain later as to what those inputs are/can be), you then run 
a script in a user-friendly statistical computing framework called bf R. This is 
where the computing (editing, imputation, etc) takes place. We will explain this in 
more detail later as well. For now, let's look closer at the <tt>spec</tt>file</tt>:</p>

<h5>The spec</tt>file</h5>
<p>The full specification of the various steps of TEA, from input of data to final 
production of output tables, that are performed during your implementation of TEA 
is specified in a single file, the em spec</tt>file. There are several intents to 
this setup. First, because the <tt>spec</tt> file is separate from programming
languages like <b>R</b> or <b>SAS</b>, it is a simpler grammar that is easier to write, and so 
analysts can write technical specifications without the assistance of a programmer or
a full training in a new programming language. In other words, the spec</tt>file allows 
users whose areas of expertise are not in programming to customize and use TEA in a 
standardized and acccessible environment. To see why this is the case, observe the 
following script that is taken from the <tt>demo.spec</tt> file (which you can open using 
Vi or any other text editor):</p>

<div class ="spec">
database: test.db
<br>
input {
<br>
        input_file :dc_pums_08.csv
<br>
        output_table: dc_pums
<br>
}
</div>

<p>In this snippet of the <tt>demo.spec</tt> file, we specified a database to use, an input 
file to be parsed, and an output table to write our imputations to. Behind the scenes, 
SQL scripts and C functions are being executed. As we will see, other scripts that are 
run from the <tt>spec</tt> file perform more complicated behind-the-scenes algorithms. 
However, before we go through an example of a full <tt>spec</tt> file, let's take a look at 
the environment and underlying systems in which TEA runs to gain a better understanding 
of the processes taking place in the <tt>spec</tt> file:</p>

<h5>Environment and underlying systems</h5>
<p>TEA is based on three systems: <b title="C is the successor to B, which was the successor to BCPL: basic combined programming language.">C</b>, <b title="R, a successor to S, is named after its authors, Robert Gentleman and Ross Ihaka.">R</b>, and <b title="SQL stands for structured query language.">SQL</b>. Each provides facilities that complement the others:</p>

<p><b>SQL</b> is designed around making fast queries from databases, such as finding all
observations within a given age range and income band. Any time we need a subset of the
data, we will use <b>SQL</b> to describe and pull it. <b>SQL</b> is a relatively simple language, so
users unfamiliar with it can probably learn the necessary <b>SQL</b> in a few
minutes--in fact, a reader who claims to know no <b>SQL</b> will probably already be able
to read and modify the SQL-language conditions in the <tt>checks sections</tt> below.</p>

<p>The TEA system stores data using an <b>SQL</b> database. The system queries the database as
needed to provide input to the statistical/mathematical components (<b>R</b>, <b>C</b> 
function libraries, etc.).
Currently, TEA is written to support <b>SQLite</b> as its database interface; however it would
be possible to implement other interfaces such as Oracle or MySQL.</p>

<p>Output at each step is also to the database, to be read as input into the next
step. Therefore, the state of the data is recorded at each step in the process, so
suspect changes may be audited.</p>

<p>Outside the database, to control what happens and do the modeling, the TEA package consists 
of roughly 5,000 lines of <b>R</b> and <b>C</b> code.</p>

<p><b>R</b> is a relatively user-friendly system that makes it easy to interact with data
sets and write quick scripts to glue together segments of the survey-processing pipeline.
<b>R</b> is therefore the interactive front-end for TEA. Users will want to get familiar with the basics of
<b>R</b>.  As with <b>SQL</b>, users well-versed in R can use their additional knowledge to perform
analysis beyond the tools provided by the TEA system.</p>

<p><b>C</b> is the fastest human-usable system available for manipulating matrices, making
draws from distributions, and other basic model manipulations. Most of the numerical
work will be in <b>C</b>. The user is not expected to know any <b>C</b> at all, because <b>R</b> procedures
are provided that do the work of running the underlying <b>C</b>-based procedures.</p>

<p>Now that we have a better idea of the environments in which TEA is run, let's take a look  
at an example of a full <tt>spec</tt> file: <tt>demo.spec</tt>:
<!--
DV:  I thought it would be a good idea to include a full spec</tt>file for the reader to 
reference while reading through the tutorial. If you guys disagree we can take it out.
Also, as we know there are problems with demo.spec</tt>so we'll have to fix those eventually 
before including it in the final draft of the tutorial.
-->

<!-- We still need to figure out how to include the entire demo.spec</tt>file without pasting the entire thing in here!-->

<!-- caption for demo.spec</tt>(when it finally gets included:
captionA sample <tt>spec</tt>file that imputes an AGEP variable using WAGP, SEX, 
and the <tt>hot deck and <tt>ols imputation models. The contents of this <tt>spec</tt>
file are explained below
endfigure
-->

<p>The configuration system (<tt>spec</tt> file) is a major part of the user interface with TEA. As is evident 
from <tt>demo.spec</tt>, there are many components of the <tt>spec</tt> file that all 
perform certain functions. 
We begin by explaining the concept of textitkeys:</p>

<h5>Keys</h5>
<p>Everything in the <tt>spec</tt> file is a key/value pair (or, as may be more familiar to 
you, a tag: data definition). Each key in the <tt>spec</tt> file has a specific purpose and 
will be outlined in this tutorial.
To begin, we start in the header of <tt>demo.spec</tt>:</p>

<div class="spec">
database: demo.db
<br>
id: SSN
</div>

<p>Here, <tt>database: demo.db</tt> and <tt>id</tt>: SSN are examples of <tt>key: value</tt> pairs.
As is the case in <tt>demo.spec</tt>, you will always need to begin your <tt>spec</tt> file by declaring the 
database (<tt>database:your_database.db</tt>) key and the unique identifier 
(<tt>id:your_unique_identifier</tt>) key. The <tt>database</tt>: key identifies 
the relational database where all of your data will be manipulated during the various processes of TEA. 
The <tt>id</tt> key provides a column in the database table that serves as the unique identifier for 
each set of data points in your data set. Though the <tt>id</tt> key is not strictly necessary, 
we strongly advise that you include one in your <tt>spec</tt> file to prevent any unnecessary frustration 
as most of TEA's routines require its use. More information on both of these keys can be found in 
the appendix of this tutorial.</p>

<p>You may have noticed that keys are assigned values with the following syntax:</p>
<tt>key: value</tt>

<p>This syntax is equivalent to the following form with curly braces:
<div class="spec">
database {
<br>
	demo.db
<br>
	 }
<br>
id	 {
<br>
	SSN
<br>
	 }
</div></p>

<p>Clearly, this form is not as convenient as the <tt>key: value</tt> form for single values. 
However, it allows us to have multiple values associated with a single line of data, and even subkeys. 
For example, take  the next line in <tt>demo.spec</tt> (the computer will 
ignore everything after a #, so those lines are comments for your fellow humans):</p>

<div class="spec">
input 
<br>
    #As above, if you have a key with one value, you can use a colon:
<br>
    input file: ss08pdc.csv
<br>
    overwrite: no
<br>
    output table: dc
<br>
	#However, with curly braces we can associate multiple values with a single key
<br>
        types 
<br>
                AGEP: integer
<br>
                CATAGE: integer
        
</div>

<p>In the database, here is what the above will look like:</p>

<div class="spec">
database            ourstudy.db
<br>
checks              age &gt; 100 =&gt; age = 100
<br>
checks              status = 'married' and  age &lt; 16
<br>
input/input file    text_in.csv
<br>
input/output table  dataset
</div>

<p>Observe that the subkeys are turned into a slash-separated list. It is 
worth getting familiar with this internal form, because when you've made a 
mistake in your <tt>spec</tt> file, the error message that gets printed in textbfR 
will display your keys in the above form. We will discuss this more later in the 
tutorial when we talk about running your <tt>spec</tt> file in textbfR.
Here is a succint summation of the syntax rules for keys:</p>

<ul>
	<li>You can have several values in curly braces, each on a separate line, which are added to the key/value list. Order is preserved.</li>
	<li>If there is a subkey, then its name is merged with the parent key via a slash;
you can have as many levels of sub-sub-sub-keys as you wish.</li>
	<li>As a shortcut, you can replace <tt>key {single value}</tt> with <tt>key:single value</tt>.</li>
	<li>Each key takes up exactly one line (unless you explicitly combine lines; see below). Otherwise, white space is irrelevant to the parser. Because humans will also read the spec</tt>file, you are strongly encouraged to use indentation to indicate what is a member of what group.</li>
	<li>If you need to continue a value over several lines, put a backslash at the end of each line that continues to the next; the backslash/newline will be replaced with a single space.</li>
</ul>

<p>We now continue through our <tt>spec</tt> file to the <tt>input</tt> key:</p>

<h5>Input declarations</h5>
<p>The <tt>input</tt> key is an important key in your <tt>spec</tt> file that specifies:</p>

<ol>
	<li>The <tt>csv</tt> file from which you will draw your data.</li>
	<li>The option to textitoverwrite the csv file currently written into the specified database with a new <tt>csv</tt> file that you have specified.</li>
	<li>The <tt>output table</tt> key that specifies the table where you would like to write the data that is read in from the <tt>csv</tt> file.</li>
	<li>The <tt>types</tt> key that specifies what type of variables are to be written into the output table.</li>
</ol>

<p>If this all seems confusing, do not fret. The layout of the <tt>spec</tt> file will make more sense 
as we continue to explain its various features. Again, more information about these keys can be 
found in the appendix.</p>

<p>We now discuss the <tt>fields</tt> key in more detail.</p>

<h5>Field declarations</h5>
<p>The edit-checking system needs to know the type and range
of every variable that has an associated edit. If a variable does not have an associated
edit, there is no need to declare its range.</p>

The declaration of the edit variables contained in the <tt>fields</tt> key consists of the field name, 
an optional type (<tt>int</tt>, <tt>text</tt>, or <tt>real</tt>) to be discussed further below, and a list of valid values. Here is a typical example:

<div class="spec">
fields 
<br>
    age: 0-100
<br>
    sex: M, F
<br>
    hh_type: int 1, 2, 4, 8
<br>
    income: real
</div>

<p>In the above code, we've declared four variables: <tt>age</tt>, <tt>sex</tt>, <tt>hh_type</tt>, and <tt>income</tt> and we've declared those four variables in different and valid ways. Declaring variable types for your 
field is necessary because the edit-checking system will know what to expect when performing 
its consistency checks later on. You can see that the list of values may be text 
or numeric, and the range 0-100 will be converted into the full sequence 0, 1, 2, ... , 100. 
By declaring our variables with a type and range, we can pass the information to the edit-checking system 
so that it knows what to verify when running its checks for each of these variables.</p>

<p>When declaring a variable, the first word following the <tt>:</tt> may be a type. For instance, 
for the <tt>hh_type: int 1, 2, 4, 8</tt> field above, we used the word <tt>`int'</tt> to indicate that 
the data values of the field were of type integer. Notice as well that a type does not necessarily 
need to be declared; in which case the default action is to treat the entry as plain text (which means
that numeric entries get converted to text categories, like <tt>"1"</tt>, <tt>"2"</tt>, ...). Keep in mind 
that if you declare the incorrect type for a field that the edit-checking system may not correctly 
verify the values of that field in your data set.</p>

<p>As a final note, we warn the user against creating fields with an overly large range of integer 
values. For a field with a range of possible integer values, the edit-checking system will verify 
that each data point falls into one of the possible values specified in the range. This can be a 
problem for a field with an excessively large range of integer values because each data point will 
have to be compared against all of the possible values in the range. Though this is easily doable for 
smaller ranges such as 0-100 or even 0-1000, it becomes extremely time consuming for larger ranges such 
as 0-600000. Instead, we recommend assigning a field with a large range as a real variable.</p>

<h6>Auto-detect</h6><p>If your field list consists of a single star, <tt>*</tt> (the wild-card character 
in <b>SQL</b> that represents all possible inputs), then the data set will be queried for the list of values 
used in the input data. All values the variable takes on will be valid; all values not used 
by the variable will not be valid. Keep in mind that this can present problems if there are errors 
in your input data set.
In any case, using <tt>*</tt> may be useful when quickly assembling a first draft of a <tt>spec</tt>, but we recommend giving an explicit set of values for production. You may precede the <tt>*</tt> with a type designation, like <tt>age: real *</tt>.</p>

<p>As you know, it is often necessary to impute data points in separate categories given 
the distribution of the data. For this, we use the <tt>recodes</tt> key.</p>

<h5>Recodes</h5>
<p>In short, <tt>recode</tt> keys are just new variables that are deterministic functions of 
existing data sets based on parameters given by you, the analyst. Based on the variables specified in the 
<tt>fields</tt> key, you can "recode" those fields into other variables (which can be thought of as categories based on paramters given in a certain syntax. This is often necessary to ensure that you are imputing 
your data over an accurate distribution. Observe the following typical example of a <tt>recodes</tt> key:</p>

<div class="spec">
recodes 
<br>
    CATAGE {
<br>
        0 | age between 0 and 15
<br>
        1 | age between 16 and 64
<br>
        2 | age &gt; 64
<br>
	   }
</div>

<p>Here, we have declared a new variable <tt>CATAGE</tt> whose data points are based off of a 
deterministic function of the variable <tt>AGEP</tt>. As we will see later, this recode will be called 
in the <tt>categories</tt> key during imputation so that the data points we are attempting to 
impute will be imputed in categories based on their recode values rather than collectively 
as a single set of data points. The <tt>recodes</tt> key is fairly straightforward, although we 
will learn about some of its more advanced features later. For now, however, we continue 
our walkthrough of the <tt>spec</tt> file by discussing the <tt>checks</tt> key.</p>

<h5>Checks</h5>
<p>The conistency-checking system is rather complex. Ironically, this complexity is what 
makes the system efficient and reliable: there are typically a
few dozen to a few hundred checks that every observation must pass, from sanity checks
like <tt>fail if age &lt; 0</tt> to real-world constraints like <tt>fail if age &lt; 16 and
status='married'</tt>. Further, every time a change is made (such as by the imputation system)
we need to re-check that the new value does not fail checks. For example, an OLS imputation of age
could easily generate negative age values, so the consistency checks are essential
for verifying that the imputation process is giving us accurate and useable data points. In addition 
to error checking we can also use consistency checks for other purposes, such as setting 
em structural zeros for the raking procedure.</p>

<p>All of the checks that are to be performed are specified here, in the <tt>checks</tt> key. 
The checks you specify here will be performed on all input variables, as well as on all 
imputed data values. Let's take a look at an example <tt>checks</tt> key:</p>

<div class="spec">
checks {
<br>
    	age &lt; 0
<br>
        age &gt; 95 =&gt; age = 95
<br>
       }
</div>

<p>In the above example, we've indicated that the consistency checking system should verify that 
age is not less than 0 and that age is not greater than 95. Notice as well that when specifying 
that <tt>age &gt; 95</tt> we've also included the line <tt>age = 95</tt> to indicate that when an age data point has a value higher than 95 that we should simply top-code it as 95. We haven't included an auto-declaration 
for <tt>age &lt; 0</tt> because if a data-point has a negative age value than it's indicative of a real error that should be properly imputed. It is up to you to decide when it is appropriate to utilize the 
auto-declaration feature.</p>

<p>We've now introduced the keys that precede the <tt>impute</tt> key. Up to this point, all of the <tt>keys</tt> we've discussed have served the purpose of preparing the data in some way to be imputed in the 
<tt>impute</tt> key. We now describe its functions below.</p>

<h5>Imputation</h5>
<p>The <tt>impute</tt> key is fairly comprehensive and has several sub-keys that fulfill various 
roles in the imputation process. Many of the values of these sub-keys are derived from values 
found earlier in the <tt>spec</tt> file; such as the fact that <tt>categories</tt> is based off of 
the variables declared in <tt>recodes</tt>. Take a look at the following example of an <tt>impute</tt> key 
that is used to outline the imputation of the age variable described in the above keys:</p>

<div class="spec">
impute {
<br>
    input table: viewdc
<br>
    min group size: 3
<br>
    draw count: 3
<br>
    seed: 2332
<br>
categories {
<br>
        CATAGE
<br>
        SEX
<br>
}    
<br>
method: hot deck
<br>
output vars: age
<br>
}
</div>

<p>As you can see, there is quite a bit going on here. Let's walk through each of the sub-keys 
above and see what they're doing:</p>

<ul>
	<li>The <tt>input</tt> table sub-key specifies the name of a view table to 
where you will write your recode variables. Recall that a view table is a digital <b>SQL</b> table 
that takes up very little memory and is very quick to render. In our example, we've indicated 
that the name of the view table to where our <tt>recode</tt> variables will be written is 
called `viewdc'. We'll be able to access this view table from <b>R</b> later when we actually 
compile our <tt>spec</tt> file.</li>
	<li><tt>min group size</tt> indicates the minimum number of known data points that can be 
used to impute any unknown data points. For instance, it wouldn't make much sense to use a single 
data point to impute five other data points as they would all end up being the same value. To 
prevent this, we set <tt>min group size: 3</tt>.</li>
	<li><tt>draw count</tt> determines the number of multiple imputations that should be performed. 
Because imputation can be done using stochastic models, it is often beneficial to obtain multiple 
imputations of the same data set. This will be explained in more detail later. For now, just know 
that the <tt>draw count</tt> sub-key determines the number of multiple imputations that are performed.</li>
	<li><tt>seed</tt> specifies the pseudo-random number generator seed that is used to obtain 
random numbers for stochastic imputation models. Keeping this seed the same will ensure that multiple imputations of the same data set will return the same outputs. Be sure to record the seeds you choose for your 
imputations so that other analysts can reference your data and replicate your imputations if necessary.</li>
	<li><tt>categories</tt> specifies which recodes will be used to impute the data set for 
the given variable. For example, if CATAGE is listed in the <tt>categories</tt> key then the data points that correspond to each of the three possible CATAGE values will be imputed separately. While it's not an absolute requirement that the <tt>categories</tt> key be implemented, most data sets are imputed by categories so you will be using this key for almost all of your imputations.</li>
	<li><tt>method</tt> specifies the type of model that is used to impute your data points. 
Choosing this model is itself a non-trivial task and is discussed more thoroughly later on 
in the tutorial.</li>
	<li><tt>output vars</tt> specifies the variable whose data points are to be imputed. For this reason 
you could consider <tt>output vars</tt> as both the `input' variable as well as the 'output' variable 
to the model specified in <tt>method</tt>.</li>
</ul>

<p>More can be found on each of the above keys in the appendix.</p>

<h5>More features</h5>
<p>This concludes our walkthrough of a typical <tt>spec</tt> file. By now you should have a basic idea of 
how a <tt>spec</tt> file is implemented within TEA. Before we continue on to explaining more about imputation, 
the models available in TEA, and other features that are available within the TEA framework, we will mention 
four more features of the <tt>spec</tt> file.

<p><h6>Group recodes</h6> The first feature that we'll mention is that of <tt>group recodes</tt>. 
Often in an imputation model, after declaring your <tt>recodes</tt>, it is also necessary to declare 
sub-recodes that are function of those earlier recodes. For example, suppose you had a <tt>spec</tt> file 
in which you intended to impute data points for various members of households. Then to perform imputations 
on:</p>

<div class="spec">
recodes {
<br>
        EARN: case when WAGP&lt;=0 then 'none' when WAGP&gt;0 then 'black' else NULL end
<br>
        MOVE: case MIG when 1 then 'moved' else 'stayed' end
<br>
        DEG: case when SCHL in ('24','23','22','21','20') then 'degree' else 'non-degreed' end
<br>
        MF: case SEX when '1' then 'Male' when '2' then 'Female' else NULL end
<br>
        REL: case RELP when '00' then 'Householder' when '01' then 'Spouse'                          when '02' then 'Child'                          when '06' then 'Parent' else NULL end
<br>
}
<br>

group recodes {
<br>
    group id column: SERIALNO
<br>
    recodes 
<br>
                NP: max(SPORDER)
<br>
        NHH: sum(RELP='00')
<br>
        NSP: sum(RELP='01')
<br>
        NUP: sum(RELP='15')
<br>
                HHAGE: max(case RELP when '00' then AGEP end)
<br>
                SPAGE: max(case RELP when '01' then AGEP end)
<br>
                SPORD: max(case RELP when '01' then SPORDER end)
<br>
                HHSEX: cast(round(avg(case RELP when '00' then SEX end)) as integer)
<br>
                SPSEX: cast(round(avg(case RELP when '01' then SEX end)) as integer)
<br>
} 
</div>

<p><h6>Including</h6> Up to this point, we've constructed the entire <tt>spec</tt> file in a single file. 
Though this approach may be appropriate in some scenarios, it's often the case that you and your colleagues would like to make edits to the same <tt>spec</tt> file and update or contruct it concurrently. To do this, you can simply mark subsidiary files to be included in the main <tt>spec</tt> file. Doing so allows you to easily combine these subsidiary files into one project through which all of TEA's routines will run.</p>

<p>The syntax for including is quite simple. To include a subsidiary file at a certain point in the <tt>spec</tt> file, simply insert the key <tt>include: textitsubsidiary_file_name</tt> at the line in the <tt>spec</tt>
file where you would like the contents of the subsidiary file to be inserted. For example, if you have 
written the consistency checks in a file named <tt>consistency</tt>, and the entire rank swapping 
configuration was in a file named <tt>swap</tt>, then you could use this parent <tt>spec</tt> file:</p>

<div class="spec">
database: test.db
<br>
include: swap
<br>
checks {
<br>
    include: consistency
<br>
}
</div>

<p>Note that any subsidiary files that you choose to include in your <tt>spec</tt> file must be in the same 
directory as the <tt>spec</tt> file or it will not be able to find them.</p>

<p><h6>Flagging for disclosure</h6> For a given crosstab, there may be cells with a small number of 
people, perhaps only one. This feature of TEA allows us to flag those cells for later handling.</p>

<p>Any combination of variables could be a crosstab to be checked, but flagging
typically focuses on only a few sensetive sets of variables. Here is the section
of the <tt>spec</tt> file describing the flagging. The <tt>key</tt> list gives the variables
that will be crossed together. With <tt>combinations: 3</tt>, every set of three
variables in the list will be tried. The <tt>frequency</tt> variable indicates that
cells with two or fewer observations will be marked.</p>

<p>We are calling this specific form of disclosure avoidance <tt>fingerprinting</tt>, so after
this segment of the <tt>spec</tt> file is in place, call <tt>doFingerprinting()</tt> from <tt>textbfR</tt> to run the procedure. The output is currently in a database table named <tt>vflags</tt>.</p>

<div class="spec">
fingerprint { 
<br>
        key {
<br>
                CATAGE
<br>
                SEX
<br>
                PUMA
<br>
                HISPF
<br>
                ESR
<br>
                PWGTP
}	     
<br>
        frequency: 2
<br>
        combinations: 3
<br>
}
</div>

<p><h6>Raking</h6> Each record in a data set might have several data items which require imputation for
different reasons:</p>

<ul>
	<li>A record could have several inconsistent items that we must replace.</li>
	<li>An otherwise consistent record could have a blank item that we wish to impute.</li>
	<li>A record could have a combination of consistent items that could lead to personal identification</li>
</ul>

<p>Each scenario implies slightly different knowledge about the data, and thus each scenario
might require a different imputation method to properly use this knowledge.</p>

<p>An overlay is a secondary data table (or set of tables) that gives information regarding
the emphreason for imputation.  Using missing data as an example, a simple overlay
could have an entry for each item in the data, indicating if that item is missing or not.
A more complicated overlay could delineate the type of non-response for each data item.</p>

<p>Raking is a method of producing a consistent table of individual cells beginning
with just the column and row totals. For this tutorial, we will use it as a
disclosure-avoidance technique for crosstabs. The column sums
and row sums are guaranteed to not change; all individual cells are
recalculated. Thus, provided the column totals have passed inspection for
avoiding disclosure, the full table passes.</p>

<p>The key inputs are the set of fields that are going to appear in the final
crosstab, and a list of sets of fields whose column totals (and cross-totals)
must not change.</p>

<p>In the <tt>spec</tt> file, you will see a specification for a three-way crosstab between
the <tt>PUMA</tt>, <tt>rac1p</tt>, and <tt>catage</tt> variables. All pairwise crosstabs
must not change, but any other details are free to be changed for the raking.</p>

<div class="spec">
raking { 
<br>
        all_vars: puma|catage | rac1p
<br>
}
<br>
contrasts {
<br>
                   catage | rac1p
<br>
                         puma | rac1p
<br>
                         puma | catage
<br>
}     
</div>

<p>As you have seen a few times to this point, once you have the <tt>spec</tt> file in place you can
call the procedure with one <tt>textbfR</tt> function, which in this case is <tt>doRaking()</tt>. But there
are several ways to change the settings as you go, so that you can experiment and adjust.</p>

<p>The first is to simply change the <tt>spec</tt> file and re-run. To do this, you will have to call <tt>read_spec</tt> again:</p>

<div class="rcode">
&gt; read_spec("demo.spec"); doRaking()    #Run two commands on a line by ending the first with a semicolon
<br>
&gt; read_spec("demo.spec"); doRaking()    #Hit the up-arrow to call up the previous line.
<br>
</div>

<p>Everything you can put in a <tt>spec</tt> file you can put on the command line. The help system
will give you the list of inputs (which will also help with writing a <tt>spec</tt> file), and you
can use those to tweak settings on the command line:</p>

<div class="rcode">
&gt; ?doRaking                   #What settings are available?
<br>
&gt; doRaking(max_iterations=2)  #What happens when the algorithm doesn't have time to
<br>
&gt; converge?
</div>

<p>This concludes our discussion of the <tt>spec</tt> file layout and syntax. At this point, you should be 
able to implement a basic <tt>spec</tt> file to impute any data set. More information about the keys 
discussed above and others that were not present in <tt>demo.spec</tt> can be found in the appendix and 
at the r-forge website.</p>

<p>We now continue our tutorial by discussing imputation in more detail.</p>

<h3>Imputation</h3>
<p>Thus far we've seen how to impute a data set using a single <tt>impute</tt> group. In this form, single 
imputation produces a list of replacement values for those data items that fail consistency checks 
or initially had no values. For the case of editing, the replacements would be for data that fails 
consistency checks; for the case of basic imputation, the replacements would be for elements that
initially had no values. Recall that as we stipulated earlier in the tutorial, for our purposes we 
consider a looser definition of imputation that includes the replacement of data points that both 
intially had no value as well as those that fail consistency checks.</p>

<p>In a similar vein, while single imputation gives us a single list of replacement values, any stochastic 
imputation method could be repeated to produce multiple lists of replacement values. For instance, 
we could utilize multiple imputation to calculate the variance of a given statistic, such as average 
income for a subgroup, as the sum of within-imputation variance and across-imputation variance.</p>

<p>To give a concrete example, consider a randomized hot deck routine, where missing values are
filled in by pulling values from similar records. For each record with a missing income:</p>

<ol>
	<li>Find the universe of which the record is a member.</li>
	<li>For each variable in turn:
		<br>
		<ul>
			<li>Make a draw from model chosen for the variable, based on the specific universe from (1).</li></ul>
	</li>
</ol>

<p>The simplest and most common example is the randomized hot deck, in which each survey respondent 
has a universe of other respondents whose data can be used to fill in the respondent's missing
values. The hot deck model is a simple random draw from the given universe for the given variable.</p>

<p>Given this framework, there are a wealth of means by which universes are formed, and
a wealth of models by which a missing value can be filled in using the data in the chosen universe.</p>

<h4>Universes</h4>
<p>The various models described above are typically fit not for the whole data set, but for
smaller <i>universes</i>, such as a single county, or all males in a given age group.
A universe definition is an assertion that the variable set for the records in the
universe was generated in a different manner than the variables for other universes.</p>

<p>An assertion that two universes have different generative processes could be construed in
several ways:</p>

<ul>
	<li>Different mathematical forms, like CART vs. logistic regression.</li>
	<li>One broad form, like logistic regression, but with different variables chosen (that is, the stochastic form is the same but the structural form differs).</li>
	<li>A unified form, like a logistic regression with age, sex, and ancestry as predictors, with different parameter estimates in each universe.</li>
</ul>

<p>Universe definitions play a central role in the current ACS edit and imputation system.
Here, universes allow data analysts to more easily specify particular courses of action in the case of missing or edit-inconsistent data items. To give an extreme example, for the imputation of marital status in ACS group quarters (2008), respondents are placed in two major universes: less than 15 years of age (U1) and 15 or more years of age (U2). The assertion here, thus, is that people older than 15 have a marital status that comes from a different generative process than those people younger than 15. This is true: people younger than 15 years of age cannot be married!  Thus in the system, any missing value of marital status in U1 can be set to ``never married'', and missing values in U2 can be allocated via the ACS hot-deck imputation system.</p>

<p>Now that we are more familar with universes, we can discuss the various models that are available in TEA and the methodology of choosing the one that is appropriate for the imputation being performed.</p>

<h3>Models</h3>
<p>Now that we're more familiar with the concept of universes, we examine how to choose the 
model that will give us the best results when imputing the data points in a specific universe.
Indeed, given an observation with a missing data point and a universe, however specified, there is
the problem of using the universe to fill in a value. Randomized hot-deck is again
the simplest model: simply randomly select an observation from the universe of acceptable
values and fill in. Other models make a stronger effort to find a somehow-optimal value:</p>

<ul>
	<li>One could find the nearest neighbor to the data point to be imputed (using any of a number of metrics).</li>
	<li>Income is typically log-Normally distributed, and log of this year's income, last year's income, and if available, income reports from administrative records (adrec ) may be modeled as Multivariate Normal (with a high correlation coefficient among variables). After estimating the appropriate Multivariate distribution for the universe, one could make draws from the distribution.</li>
	<li>If the Multivariate Normal seems implausible, one could simply aggregate the data into an empirical multivariate PDF, then smooth the data into a kernel density estimator (KDE). Draws from a specified KDE can be made as easily as draws from a standard Multivariate Normal.</li>
	<li>Count data, such as the number of hospital visits over a given period, are typically modeled as a Poisson distribution. In a manner similar to fitting a Normal, on could find the best-fitting Poisson distribution and draw from that distribution to fill in the missing observation.</li>
	<li>Bayesian methods: if adrec are available, they may be used to generate a prior distribution on a variable, which is then updated using non-missing data from the survey.</li>
	<li title="Note that OLS assumes a Normally-distributed error term and therefore gives a distribution of values for the predicted income, not just a single number; therefore one could draw multiple imputations.">One could do a simple regression using the data on hand. For example, within the universe, regress income on  age, race, sex, and covariates from adrec. Once the regression coefficients are calculated, use them to impute income for the record as the predicted value plus an error term.</li>
	<li>Discrete-outcome models, like the Probit or Logit, could be used in a similar manner.</li>
	<li>To expand upon running a single regression, one can conduct a structured search over regression models for the best-fitting regression.footnoteBecause the imputation step is not an inference step, such a search presents few conceptual difficulties. Such a search is currently in use for disclosure avoidance in ACS group quarters.</li>
</ul>

<p>A unified framework would allow comparison across the various imputation schemes and structured tests of the relative merits of each. Though different surveys are likely to use different models for step (2) of the above process, the remainder of the overall routine would not need to be rewritten across surveys.</p>

<p>We now discuss each of the models that are available in TEA.</p>


<!-- DV: Remember that we need to add rel to this list. -->
<h4>Models of TEA</h4>
<p>This section describes the models available for use in imputing missing data.</p>

paragraphHot deck This is randomized hot deck. Missing values are filled in by
drawing from nonmissing values in the same subset. The assumption underlying the model
is em missing completely at random (MCAR), basically meaning that nonrespondents
do not differ in a systematic way from respondents.
par
This model has no additional keys or options, although the user will probably want an
extensive set of category subsets. Example:
par
beginlstlisting
impute
    input table: dc
    min group size: 3
    draw count: 5
    id: serialno
                                                                             905,1         76categories 
        agecat
        sex
    
    output vars: sex
    method: hot deck

par
impute
    input table: dc
    min group size: 3
    draw count: 5
    id: serialno
par
categories 
        agecat
        sex
    
    method: ols
                                                                             923,1         78utput vars: agep
    input vars: rac1p, nativity||sex

endlstlisting
par
paragraphOrdinary least squares (aka regression)  This is the familiar <!-- MATH
 $y = beta_0
+ beta_1 x_1  + beta_2 x_2 + ... +epsilon$
 -->
<IMG
 WIDTH="41" HEIGHT="15" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$y = beta_0
+ beta_1 x_1 + beta_2 x_2 + ... +epsilon$"> form. The parameters (including the
variance of <IMG
 WIDTH="328" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$epsilon$">) are estimated for the subset of the category where all variables
used in the regression are not missing. For a point where <IMG
 WIDTH="52" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img4.png"
 ALT="$y$"> is missing but <!-- MATH
 $(x_1, x_2, ...)$
 -->
<IMG
 WIDTH="10" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img5.png"
 ALT="$(x_1, x_2, ...)$"> are not,find <!-- MATH
 $beta_1 x_1  + beta_2 x_2 + ...$
 -->
<IMG
 WIDTH="90" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img6.png"
 ALT="$beta_1 x_1 + beta_2 x_2 + ...$">, then add a random draw from the distribution of <IMG
 WIDTH="328" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$epsilon$">.
par
The variables may be specified via the usual SQL, with two exceptions to accommodate the
fact that so many survey variables are categorical.
typical meaning.
par
comment 
Because every dependent variable must be complete for the regression to calculate the
independent variable, it makes sense to put regressions in a sequence, with a spec</tt>file like:
par
beginlstlisting
impute (first)
        models
        rac1p  method: hot deck 
        nativity  method: hot deck 
        sex  method: hot deck 
        

par
impute (second)
        models
        agep  method: ols
               vars:  rac1p, nativity||sex
        
        

endlstlisting
par
and then two steps in R:
par
beginlstlisting
doMImpute(first)
doMImpute(second)
endlstlisting

par
paragraphProbit and Logit These models behave like OLS, but are aimed at categorical
variables. The output variable is not restricted to twoc categories.
par
paragraphseqRegAIC
par
paragraphDistributions The Normal (aka Gaussian) distribution is the archetype of this
type of model. First, estimate the mean and standard deviation using the non-missing data
in the category. Then fill in the missing data via random draws from the Normal
distribution with the calculated parameters. You may use either <tt>method: normal or
<tt>method: gaussian.
par
For other situations, other distributions may be peferable. For example, income is
typically modeled via <tt>method: lognormal. Count data may best be modeled via tt
method: poisson.
par
Hot deck is actually a fitting of the Multinomial distribution, in which each bin has
elements in proportion to that observed in the data; <tt>method: hot deck and tt
method: multinomial are synonyms.
par
The em method: multivariate normal doesn't work yet.
par
The distribution models have no additional options or keys.
par
paragraphKernel smoothing A kernel density estimate overlays a Normal distribution over
every data point. For example, if a data set consisted of two data points at 10 and one at
12, then there would be a large hump in the final distribution at 10, a smaller hump at
12, and the distribution would be continuous over the entire space of reals.
par
Thus, kernel smoothing will turn a discrete distribution consisting of values on a
few values into a continuous distribution.
par
Invoke this model using either <tt>method: kernel or <tt>method: kernel density.
par
todo We have written but not incorporated an HTML viewer for data using overlay
information.
par
paragraphInternals
The Census Bureau often deals with tables that would have 1.5 million cells if written
out in full, but where only maybe 20,000 surveys are in hand. Therefore, the algorithm
is heavily oriented toward sparse data.
par
It begins with a long and tedious routine to write SQL to generate the set of
possibly-nonzero values, as per the example above. SQL is the appropriate language
for generating this list because it is optimized for generating the cross of several
variables and for pruning out values that match our criteria. The tedium turns out
to be worth it: our test data set takes about 25 seconds to run using the original
full-cross `72 algorithm, and runs in under two seconds using the SQL-pruned matrix.
par
comment 
sectionSynthesis
Group quarters data in the 2010 Decennial census will be protected via
a method known as ``synthetic data''. This method uses statistical models to modify
records in need of confidentiality protection. Such a method has already been applied to
group quarters records in the American Community Survey for collection years 2006-2009.
par
sectionWeighting
We use the em survey package for R to calculate and modify weights for a
survey. The package is fully documented in citetlumley:surveys.
par
sectionModels and Universes
Statistical models are, in essence, an assertion about the emphprocess that generates
the data. As such, useful models for imputation are those that imply processes that
we believe generate ACS GQ data items. Models are typically prescribed at the level
of variables; that is, we seek to describe the generative process of a set of variables
for a certain population.
par
A given variable set could have several distinct processes by which its values
are generated. In this case, it would be suboptimal to use only a single model to
perform imputation for this variable set. We group respondent records together into a
``universe'' when we believe their values for a given set of variables are generated
by the same process. Different sets of variables that require imputation could thus
require different universe definitions.
par
subsectionModels
Here, we list several models used to predict or fill in a missing element of
a record. The element could be missing because it was not reported (imputation),
because it was incorrectly reported (editing), or because leaving the correct value
as it lays would create disclosure risk. For any of these cases, any of the models to
follow could be applied. Note that these models are typically inserted into a larger
procedure; for example, a series of models could be used to fill in various data items.
par
bf Note on MI: to be ``proper'' MI models, they would need to produce
posterior predictive densities, so we might mention that deterministic
procedures don't fit the bill
par
subsubsectionRandomized hot deck/multinomial This is a univariate method, that
randomly draws a value from the list of nonmissing values for the variable.
bf you can have a multivariate hot deck: we could get a value of A and B
from a cell defined by C and D
par
subsubsectionNearest-neighbor Based upon some metric defined by the user, find the
record closest to the record to be filled in, and copy the value(s) from that
record.
par
subsubsectionProbabilistic nearest-neighbor Define the odds of selecting a record
as inversely proportional to the record's distance to the record to be filled
in. That is, the nearest neighbor is the most likely to be selected, but others
may also be selected. After selecting a record, copy the value(s) as with
the deterministic nearest-neighbor model.
par
subsubsectionLinear models Given a complete-data subset, fit a generalized linear
model (GLM) to be specified by the user. The predictors used for a model could
be selected by hand or could be chosen automatically based on a given
criterion, such as the Akaike/Bayesian Information Criterion.
bf we could also suggest sequential models that start of predicting based
upon complete items; that is, if V1 is complete in the subset, first do
V2 ~ V1, then V3 ~ V1 + V2, etc; if nothing in complete, we could
always pick one to fill in randomly (that is, just use a prior)
par

par
Recall that we had briefly discussed multiple imputation in the <tt>impute section above. 
We now discuss this in more detail.
par
sectionMultiple Imputation
A single imputation would produce a list of replacement
values for certain data items. Any stochastic imputation method could be repeated
to produce multiple lists of replacement values. Variance of a given statistic,
such as average income for a subgroup, is then the sum of within-imputation variance and
across-imputation variance.
par
beginitemize
tighten 
item For each imputation:
    beginitemize
tighten 
    item fill in the data set using the given set of imputed values
    item calculate the within-imputation variance
    enditemize
item Sum the across-imputation and average within-imputation variances to
produce an overall variance figure that takes into account uncertainty due to imputation.
enditemize
par
The question of what should be reported to the public from a sequence of imputations
remains open. The more extensive option would be to provide multiple data sets; the less
extensive option would be to simply provide a variance measure for each data point that
is not a direct observation.
par
ifimputation
        paragraphInterface Figure refsippconfig shows a (slightly abbreviated)
        configuration file describing the hot deck process for a variable in the SIPP.
        It is intended to be reasonably readable, and maintainable by an analyst who is
        a statistician but not a programmer.
par
Lines 11-18 of the configuration specify the
        categories used for step (1) above: draws are made from the universe of records with an
        age in the same age category as the record to be filled in and <tt>num_sipp_jobs_2008 in the same
        category as well.
par
Of course, different surveys would have different classification schemes, but
        this means that each survey would need a new configuration file, not new code.
par
Line eight indicates that five imputations are to be done for each missing
        value. Those with extensive experience with multiple imputation often advise
        that a handful of imputations are sufficient for most purposes.
par
The sample from
        Figure refsippconfig focused on the determination of categories in which to
        do imputation. Figure refacsconfig focuses on regression models that go
        beyond the simple randomized hot deck of Figure refsippconfig. Lines 5-8
        specify the variables that need imputation, and the form of model to be used.
        The current system will search the set of models of the given form for the one
        that best fits the known data; Lines 9-14 show the list of variables that could be
        used as explanatory variables, although a typical model will likely wind up
        using only around four or five.
par
paragraphEdits The system as written includes a component that checks consistency
        against a sequence of edits. In line three of the sample spec</tt>file of Figure
        refsippconfig, the <tt>flagearn variable is declared to have possible values of 0,
        1, 3, or 4, but line four specifies that if an imputation returns 4, then it is rejecte
d.
        The imputation routine sketched above does not need to include any edit rules, because
        this edit step will take those into account; the separation of edits and imputations
        simplifies the routine.
      beginfigure
        beginlstlisting[language=,numbers=left,numberstyle=scshape]
        database: sipp.db
par
|flagearn 0, 1, 3, 4
        flagearn = 4
par
impute_by_groups 
            min_group_size  20
            iteration_count  5
            datatab  sippdata
par
categories 
               15&lt;=agesipp200812&lt;18;
               18&lt;=agesipp200812&lt;22;
               22&lt;=agesipp200812&lt;40;
               40&lt;=agesipp200812&lt;62;
               62&lt;=agesipp200812;
               num_sipp_jobs_2008 = 0;
               num_sipp_jobs_2008 = 1;
               num_sipp_jobs_2008 =&gt; 2;
         
par
imputes
               flagearn&nbsp; flagearn;
            ;
        
        endlstlisting
        captionA sample configuration file, for use in hot deck imputation of the em
        has earnings flag of the SIPP. 
        labelsippconfig
        endfigure
par
beginfigure
        beginlstlisting[language=,numbers=left,numberstyle=scshape]
        database: acs2008.db
par
impute
            seqRegAIC
                vars
                  TI model: gam 
                    DIS model: multinom 
                
                predlist #Sample of predictors that could used for regressions
                    SEX; YOE; WKL; MIL; UR;
                    SCH; RCGP; POV; SS; MAR;
                    LANX; JWTR; TYPGRP; GQINST; FER;
                    ESR; DIS; COW; CIT; OCC2;
                
            
        
par
endlstlisting
        captionA sample configuration file for imputation of values for ACS GQ
        disclosure avoidance. A Generalized Additive Model is fit for total income (TI),
        and a polytomous regression is fit for disability status (DIS). 
        labelacsconfig
        endfigure
else fi
par
sectionInteractive R
The specification file described to this point does nothing by itself, but provides
information to procedures written in textbfR that do the work. In fact, TEA is simply 
called as a library in textbfR, and the <tt>spec</tt>file itself is instantiated by 
issuing commands from the textbfR command prompt described below.
par
subsectionLoading TEA in R
When you start R (from the directory where the data is located), you are left at a
simple command prompt, &gt;, waiting for your input. TEA extends R via a library of
functions for survey processing, but you will first need to load the library, with:
beginlstlisting[language=]
&gt; library(tea)
endlstlisting
par
[You can cut crimson-bordered code blocks and paste them directly onto the textbfR
command line, while blue-bordered blocks are spec</tt>file samples and
would be meaningless typed out at the R command prompt.]
par
Now you have all of the usual commands from textbfR, plus those from TEA. You only
need to load the library once per textbfR session, but it's harmless if you run
<tt>library(tea) several times.
par
After loading the library by running <tt>&gt; library(tea), you would then need to 
tell textbfR to read your <tt>spec</tt>file, perform the checks, perform the imputations, 
and then finally check out the imputations to an textbfR data structure so that you 
can view them. Observe the following code:
par
beginlstlisting[language=]
&gt; library(tea)
&gt; read_spec("spec")
&gt; doChecks()
&gt; doMImpute()
&gt; checkOutImpute()
endlstlisting
par
These commands could be entered in a script file as easily as they're entered on R's command line.
You always have the option of creating a <tt>.R file that has each of the above commands listed 
sequentually. Observe the following example of a <tt>.R file:
par
beginlstlisting[language=]
library(tea)
library(ggplot2)
readSpec("demo.spec")
doChecks()
doMImpute()
checkOutImpute()
endlstlisting
par
If we assume that the file above is named <tt>demo.R then we could run the following command 
from textbfR's command line:
par
beginlstlisting[language=]
&gt; source("demo.R")
endlstlisting
par
Then textbfR would automatically run each of the scripts specified in <tt>demo.R and would 
accomplish the exact same thing as running each command separately through textbfR's command line.
Though running <tt>&gt; source("your_file.") is often quicker and more convenient than entering 
each command separately on the command line, entering the commands separately can often aid in 
verifying the results of the consistency-checking step, verifying the results of the imputation, 
et cetera.
<BR>
par
In either case, once your <tt>spec</tt>file has been correctly implemented, your data will be 
imputed and available for viewing through an textbfR data-frame.
We examine how this is done in the next subsection.
par
subsectionShowing the data
Data is stored in two places: the database, and R data frames. Database tables
live on the hard drive and can easily be sent to colleagues or stored in
backups. R data frames are kept in R's memory, and so are easy to manipulate and
view, but are not to be considered permanent. Database tables can be as large as
the disk drive can hold; R data frames are held in memory and can clog up memory if
they are especially large.
par
You can use TEA's <tt>show_db_table function to pull a part of a database table into
an R data frame. You probably don't want to see the whole table, so there are
various options to limit what you get. Some examples:
beginlstlisting[language=]
&gt; teaTable("dc", cols="AGEP, PUMA", where="PUMA=104")
&gt; teaTable("dc", cols="AGEP, PUMA", limit=30, offset=100)
endlstlisting
par
The first example pulls two columns, but only where <tt>PUMA=104. The second
example pulls 30 rows, but with an offset of 100 down from the top of the
table. You will probably be using the <tt>limit often; the <tt>offset allows
you to check the middle of the table, if you suspect that the top of the table
is not representative.
par
In fact, there are still more options. Rather than listing them all here, you can get them
via R's help system:
beginlstlisting[language=]
&gt; ?teaTable
endlstlisting
par
This <tt>?name form should give you a help page for any function, including TEA
functions like <tt>?doRaking or <tt>?doMImpute. (Yes, TEA function documentation is still a
little hit-and-miss.) Depending on R's setup, this may start a paging program that lets 
you use the arrow keys and page up/page down keys to read what could be a long document. 
Quit the pager with <tt>q.
par
The <tt>show_db_table function creates an R data frame, and, because the examples above didn't do
anything else with it, displays the frame to the screen and then throws it out. Alternatively, you can
save the frame and give it a name. R does assignment via <tt><IMG
 WIDTH="173" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$&lt;$">-, so name the output
with:
par
beginlstlisting[language=]
&gt; p104 &lt;- teaTable("dc", cols="AGEP, PUMA", where="PUMA=104")
endlstlisting
par
To display the data frame as is, just give its name at the command line.
par
beginlstlisting[language=]
&gt; p104
endlstlisting
par
But you may want to restrict it further, and R gives you rather extensive control over
which rows and columns you would like to see.
par
Another piece of R and spec</tt>file syntax: the <tt># indicates a comment to the human
reader, and R will ignore everything from a <tt># to the end of the line.
par
beginlstlisting[language=]
&gt; p104[1,1]          #The upper-left element
&gt; p104[1,"AGEP"]     #The upper-left element, using the column name
&gt; p104[ ,"AGEP"]     #With no restriction on the rows, give all rows--the full AGEP vector
&gt; p104[17, ]         #All of row 17
par
&gt; minors &lt;- p104[, "AGEP"] &lt; 18
&gt; minors             #A true/false vector showing which rows are under 18.
&gt; p104[minors, ]     #You can use that list of true/falses to pick rows. This gives all rows under 18yo.
endlstlisting
par
These commands could be entered on R's command line as easily as in a script file, so an
analyst who needs to verify the results of the consistency-checking step could copy and
paste the first three lines of the script onto the R command prompt, where they will run and
then return the analyst to the command prompt, where he or she could print subsections of
the output tables, check values, modify the spec</tt>files and re-run, continue to the
imputation step, et cetera.
par
sectionConclusion
This concludes our tutorial. If you have any questions or comments please feel free to contact 
us at xxxxxxxxx@email.gov. Also, check out the R-Forge Website at 
urlhttps://r-forge.r-project.org/projects/tea/ for updates to both the TEA software as well 
as this tutorial document.
par
section*Appendix: keys
par
This is a reference list of all of the available keys that could appear in a spec</tt>file.
As a reference, descriptions are brief and assume you already know the narrative of the
relevant procedures, in the main text.
par
Keys are listed using the <tt>group/key/value notation described in Section refspecsec. As described there, one could write a key as either
beginlstlisting[language=]
group 
    key : value

par
#or as
group 
    key 
        value
    

endlstlisting
par
Here are the keys, in alphabetical order.
par
 setlengthparsep0pt hspace-0.5cm textbfraking/thread count:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 You can thread either on the R side among several tables,
     or interally to one table raking. To thread a single raking process, set this to the
     number of desired threads. 
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/primary key:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 A list of variables to use as the primary key for the output table.
In SQLite, if there is only one variable in the list as it is defined as an integer,
this will create an integer primary key and will thus be identical to the auto-generated
ROWID variable.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfgroup recodes/recodes:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 A set of recodes like the main set, but each calculation of the recode will be grouped by the group id, so you can use things like <tt>max(age) or <tt>sum(income).
Returns 0 on OK, 1 on error.
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf    raking/tolerance:
<BR>
setlengthparindent0pt setlengthparsep3pt 
     If the max(change in cell value) from one step to the next
       is smaller than this value, stop. 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/types:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 A list of emphkeys of the form:
var: type
where var is the name of a variable (column) in the output table and type is a valid
database type or affinity.  The default is to read in all variables as character
columns.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfid:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 Provides a column in the data set that provides a unique identifier for each
observation.
Some procedures need such a column; e.g., multiple imputation will store imputations in a
table separate from the main dataset, and will require a means of putting imputations in
their proper place. Other elements of TEA, like flagging for disclosure avoidance, use the
same identifier.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfrankSwap/max change:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 maximal absolute change in value of x allowed.
That is, if the swap value for <IMG
 WIDTH="14" HEIGHT="22" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$x_i$"> is <IMG
 WIDTH="52" HEIGHT="27" ALIGN="MIDDLE" BORDER="0"
 SRC="img4.png"
 ALT="$y$">, if <IMG
 WIDTH="16" HEIGHT="18" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$\vert y - x_i\vert &gt;$"> maxchange,
then the swap is rejected
par
default = 1
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf    raking/all vars:
<BR>
setlengthparindent0pt setlengthparsep3pt 
     The full list of variables that will be involved in the
       raking. All others are ignored. 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf  impute/draw count:
<BR>
setlengthparindent0pt setlengthparsep3pt 
   How many multiple imputations should we do? Default: 5.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfrankSwap/seed:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The random number generator seed for the rank swapping setup.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf  impute/earlier output table:
<BR>
setlengthparindent0pt setlengthparsep3pt 
   If this imputaiton depends on a previous one, then give the fill-in table from the previous output here.
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfimpute/input table:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The table holding the base data, with missing values. 
  Optional; if missing, then I rely on the sytem having an active table already recorded. So if you've already called <tt>doInput() in R, for example, I can pick up that the output from that routine (which may be a view, not the table itself) is the input to this one. 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf  impute/output table:
<BR>
setlengthparindent0pt setlengthparsep3pt 
   Where the fill-ins will be written. You'll still need <tt>checkOutImpute to produce a completed table.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf  impute/seed:
<BR>
setlengthparindent0pt setlengthparsep3pt 
   The RNG seed
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfraking/contrasts:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The sets of dimensions whose column/row/cross totals must be kept constant. One contrast to a row; pipes separating variables on one row.
beginlstlisting
raking
    contrasts
        age | sex | race
        age | block
    

endlstlisting 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/indices:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 Each row specifies another column of data that needs an index. Generally, if you expect to select a subset of the data via some column, or join to tables using a column, then give that column an index. The <tt>id column you specified at the head of your spec</tt>file is always indexed, so listing it here has no effect.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfrankSwap/swap range:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 proportion of ranks to use for swapping interval, that is
if current rank is r, swap possible from r+1 to r+floor(swaprange*length(x))
par
default = 0.5
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf    raking/count col:
<BR>
setlengthparindent0pt setlengthparsep3pt 
     If this key is not present take each row to be a
		single observation, and count them up to produce the cell counts to which the
		system will be raking. If this key is present, then this column in the data set
		will be used as the cell count.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/input file:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The text file from which to read the data set. This should be in
the usal comma-separated format with the first row listng column names.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfrecodes:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 New variables that are deterministic functions of the existing data sets.
There are two forms, one aimed at recodes that indicate a list of categories, and one
aimed at recodes that are a direct calculation from the existing fields.
For example (using a popular rule that you shouldn't date anybody who is younger than
(your age)/2 +7),
beginlstlisting[language=]
recodes  
    pants 
        yes | leg_count = 2
        no  |                   #Always include one blank default category at the end.
    
    youngest_date 
        age/2 + 7
    

endlstlisting
You may chain recode groups, meaning that recodes may be based on previous recodes. Tagged
recode groups are done in the sequence in which they appear in the file. [Because the
order of the file determines order of execution, the tags you assign are irrelevant, but
I still need distinct tags to keep the groups distinct in my bookkeeping.]
beginlstlisting
recodes [first] 
    youngest_date: (age/7) +7        #for one-line expressions, you can use a colon.
    oldest_date: (age -7) *2

recodes [second] 
    age_gap 
        yes | spouse_age &gt; youngest_date &amp;&amp; spouse_age &lt; oldest_date
        no  | 
    

endlstlisting
If you have edits based on a formula, then I'm not smart enough to set up the edit table
from just the recode formula. Please add the new field and its valid values in the c
fields section, as with the usual variables.
If you have edits based on category-style recodes, I auto-declare those, because the
recode can only take on the values that you wrote down here.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfraking/input table:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The table to be raked. 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/missing marker:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 How your text file indicates missing data. Popular choices include "NA", ".", "NaN", "N/A", et cetera.
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbftimeout:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 Once it has been established that a record has failed a consistency
   check, the search for alternatives begins. Say that variables one, two, and three each have 100
    options; then there are 1,000,000 options to check against possibly thousands
    of checks. If a timeout is present in the spec</tt>(outside of all groups), then the
    alternative search halts and returns what it has after the given number of seconds
    have passed.
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf    raking/max iterations:
<BR>
setlengthparindent0pt setlengthparsep3pt 
     If convergence to the desired tolerance isn't 
       achieved by this many iterations, stop with a warning. 
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/output table:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The name of the table in the database to which to write the data read in.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfdatabase:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The database to use for all of this. It must be the first thing on your line.
I need it to know where to write all the keys to come.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbf    raking/run number:
<BR>
setlengthparindent0pt setlengthparsep3pt 
     If running several raking processes simultaneously via
        threading on the R side, specify a separate run_number for each. If
        single-threading (or if not sure), ignore this.
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/overwrite:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 If <tt>n or <tt>no, I will skip the input step if the output table already exists. This makes it easy to re-run a script and only sit through the input step the first time.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfgroup recodes:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 Much like recodes (qv), but for variables set within a group, like
eldest in household.
For example,
beginlstlisting[language=]
group recodes  
    group id : hh_id
    eldest: max(age)
    youngest: min(age)
    household_size: count(*)
    total_income: sum(income)
    mean_income: avg(income)

endlstlisting
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfraking/structural zeros:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 A list of cells that must always be zero, 
     in the form of SQL statements. 
par
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfinput/primarky key:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The name of the column to act as the primary key. Unlike other indices, the primary key has to be set on input.
endkey
par
 setlengthparsep0pt hspace-0.5cm textbfgroup recodes/group id:
<BR>
setlengthparindent0pt setlengthparsep3pt 
 The column with a unique ID for each group (e.g., household number).
endkey
par
enddocument
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"></A>

<UL>
<LI><A NAME="tex2html4"
  HREF="node1.html">About this document ...</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html2"
  HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev_g.png">   
<BR>
<B> Next:</B> <A NAME="tex2html3"
  HREF="node1.html">About this document ...</A>
<!--End of Navigation Panel-->
<ADDRESS>
verne301
2013-03-18
</ADDRESS>
</BODY>
</HTML>
